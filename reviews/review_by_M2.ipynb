{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dae50f9",
   "metadata": {},
   "source": [
    "# Review by Team Member 2\n",
    "\n",
    "## Task Summary\n",
    "I was responsible for writing the `download_answer_files` and `collate_answer_files` functions. \n",
    "The purpose was to retrieve and rename raw answer files from the cloud (Google Drive), and combine all respondent \n",
    "data into a unified file respectively. On top of this I included a function called `convert_to_direct_link`, the \n",
    "reason for this was that after mass selecting and copying the links on Google Drive i found that they were in an \n",
    "incorrect format after downloading, resulting in the files being saved to the data folder and subsequent output folder \n",
    "incorrectly. I had to change the output from HTML text.\n",
    "\n",
    "## Code Review\n",
    "The code works by inputting the URLs as a string, converting it to a list and then iteratively going through and editing \n",
    "each link so that its not in view mode. I then created the `download_answer_files` function, in which i checked that the\n",
    "data folder existed, if not it would be created. I then downloaded the file using the google download module, saving it \n",
    "to the data folder with the required name. I used a for loop to call this function for the number of respondents. \n",
    "Finally, I created the `collate_answer_files` function to combine all the files in the data folder as one file. First, i saved the location in which i wanted the combined file to go to as a variable, then I opened the output file in write mode. From there I then iterated through the filenames in the data folder, checking that it wasnt the temporary file (Now deleted. It was used as a temporary file to create the folder). I then opened the file and inputted it into the `collated_answers.txt`, with a * between each file.\n",
    "\n",
    "I think this method was largely efficient, however reproducibility could be an issue with larger numbers of files, as i had to \n",
    "manually select each file to then bulk get all the links. A function could be added that scrapes the links instead, however for 25 links the current method works. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
